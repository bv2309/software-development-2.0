https://github.com/bv2309

{"codex_prompt_version":"1.0","project_name":"ai_accel_api_platform","goal":"Create an industry-standard Python backend platform that (1) automatically selects GPU (CUDA) / Apple MPS / CPU for compute, (2) uses an AI-accelerated database layer (vector embeddings + ANN search) to speed up retrieval, and (3) serves an optimal API for web + mobile clients (REST/OpenAPI, optional gRPC). Include C/C++ acceleration hooks for hot paths and optional GPU kernels via the chosen ML runtime.","decisions":{"language":"Python 3.12","package_manager":"uv","api_framework":"FastAPI (async) + Uvicorn","db":"PostgreSQL 16 + pgvector","cache":"Redis","background_jobs":"RQ (simple) or Celery (optional); default to RQ","ml_runtime":"PyTorch (device auto-detect: CUDA/MPS/CPU)","cpp_acceleration":"pybind11 + CMake (+ optional Torch C++ extensions when beneficial)","deployment":"Docker Compose for local; container-ready for prod","observability":"structlog + OpenTelemetry hooks + Prometheus metrics endpoint","auth":"OAuth2 Password/JWT for first iteration; pluggable for external IdP"},"non_goals":["No UI work; provide API only","No proprietary external services required; keep runnable locally","No heavy multi-tenant enterprise features unless explicitly requested later"],"repository_layout":{"root":["README.md","pyproject.toml","uv.lock","Dockerfile","docker-compose.yml",".env.example",".gitignore",".editorconfig",".pre-commit-config.yaml","Makefile","proto/","scripts/","src/","tests/","docs/","migrations/"],"src/":["ai_accel_api_platform/**init**.py","ai_accel_api_platform/main.py","ai_accel_api_platform/settings.py","ai_accel_api_platform/logging.py","ai_accel_api_platform/api/","ai_accel_api_platform/core/","ai_accel_api_platform/db/","ai_accel_api_platform/ai/","ai_accel_api_platform/workers/","ai_accel_api_platform/telemetry/","ai_accel_api_platform/cpp/"],"src/ai_accel_api_platform/api/":["**init**.py","deps.py","v1/","middleware.py"],"src/ai_accel_api_platform/api/v1/":["**init**.py","routes_health.py","routes_items.py","routes_embeddings.py","routes_search.py","routes_auth.py"],"src/ai_accel_api_platform/core/":["**init**.py","device.py","errors.py","security.py","schemas.py","utils.py"],"src/ai_accel_api_platform/db/":["**init**.py","session.py","models.py","repositories.py","vector.py"],"src/ai_accel_api_platform/ai/":["**init**.py","embeddings.py","retrieval.py","rerank.py"],"src/ai_accel_api_platform/workers/":["**init**.py","rq_worker.py","tasks.py"],"src/ai_accel_api_platform/telemetry/":["**init**.py","metrics.py","tracing.py"],"src/ai_accel_api_platform/cpp/":["CMakeLists.txt","bindings.cpp","fast_ops.cpp","fast_ops.h"],"proto/":["search.proto"],"migrations/":["README.md"],"tests/":["test_health.py","test_device.py","test_embeddings.py","test_search.py"]},"implementation_instructions":["Initialize repo with uv and Python 3.12. Create pyproject.toml using PEP 621. Add strict tooling and reproducibility: ruff (lint+format), mypy, pytest, coverage, pre-commit, and a GitHub Actions CI workflow that runs ruff, mypy, pytest on push/PR.","Create FastAPI service with clean layering: api (routes), core (settings/device/security), db (models/session/repos/vector), ai (embeddings/retrieval/rerank), workers (async job execution), telemetry (metrics/tracing). Use Pydantic Settings for configuration; load from env; provide .env.example.","Device auto-detection: implement src/.../core/device.py with a single function get_best_device(prefer_gpu=True) that returns a torch.device and a short descriptor string. Logic: if torch.cuda.is_available() -> cuda; elif torch.backends.mps.is_available() -> mps; else cpu. Also expose get_device_info() that returns JSON-serializable details (device type, name, torch/cuda versions when present). Add unit tests that monkeypatch torch availability to validate selection order.","AI acceleration in DB: implement embeddings pipeline + vector search. Use pgvector extension with SQLAlchemy models: Item(id UUID, content TEXT, metadata JSONB, embedding VECTOR(d)). Provide migrations instructions (Alembic) and a docker-compose Postgres service that enables pgvector (either via pgvector image or init script). Create repository methods: upsert_item_with_embedding, vector_search(query_embedding, top_k, filters), and hybrid_search(optional text filter + vector ANN).","Embeddings service: implement ai/embeddings.py that loads a sentence-transformers model (default: all-MiniLM-L6-v2 unless overridden by env) onto best device. Provide async-safe singleton initialization (avoid re-loading per request). Ensure batching support and torch.no_grad() inference. Add optional quantization or torch.compile toggle via env, guarded by availability.","Optional reranking: implement ai/rerank.py as a lightweight cross-encoder reranker (disabled by default; behind env flag). If disabled, return vector search ranking only. Keep it modular so it can be swapped later.","C/C++ acceleration hooks: create a pybind11 extension under src/.../cpp. Implement a small example hot-path function (e.g., fast cosine similarity / dot-product batch) in C++ with OpenMP where available. Provide Python wrapper in core/utils.py or ai/retrieval.py. Build via scikit-build-core or setuptools+cmake; prefer scikit-build-core for modern builds. Ensure the project still works without building the extension (pure Python fallback). Document build flags and provide a Makefile target `make build-cpp`.","GPU kernel acceleration approach: do not write custom CUDA/Metal kernels initially; instead leverage PyTorch which already routes ops to CUDA/MPS/CPU. Keep a clear seam (ai/embeddings.py + ai/retrieval.py) where future CUDA/C++ extensions can be added. Provide a placeholder doc explaining how to add Torch extensions for CUDA later.","API design (web+mobile): implement versioned REST API under /v1 with OpenAPI automatically. Endpoints: GET /v1/health (includes device info, db ping), POST /v1/items (create/update item, optionally enqueue embedding job), GET /v1/items/{id}, POST /v1/embeddings (return embedding for text), POST /v1/search (vector/hybrid search), POST /v1/auth/token (JWT). Use consistent request/response schemas in core/schemas.py.","Optional gRPC: add proto/search.proto defining SearchService with Search and Upsert RPCs. Provide optional server wiring guarded by env flag; do not block REST if gRPC is off. Generate stubs in CI if enabled. Keep it minimal and documented; prioritize REST for default.","Background processing: implement Redis + RQ worker. If POST /v1/items is called with async_embedding=true, enqueue a job to compute and store embedding; otherwise compute inline. Provide worker entrypoint and docker-compose service for the worker.","Database + caching: configure async SQLAlchemy engine for Postgres (asyncpg). Add Redis cache for frequent search queries keyed by normalized query+filters with short TTL. Ensure cache invalidation on item updates by namespace versioning.","Security: implement JWT auth with OAuth2 password flow for initial version (single user store in DB). Add CORS config for web/mobile. Add rate limiting middleware (simple in-memory for dev; Redis-based for prod).","Observability: add structlog JSON logging. Add /metrics endpoint (Prometheus) and trace hooks (OpenTelemetry optional). Include request id middleware.","Docs: README with quickstart (uv sync, docker compose up, run api, run worker), environment variables, device selection behavior, how embeddings + pgvector work, and how to call endpoints. Add docs/architecture.md with module overview and extension points (C++/CUDA later).","Testing: create pytest suite for health, device selection, embeddings (mock model), vector search repository (use testcontainers or docker-compose service in CI). Keep unit tests fast; mark integration tests separately.","Code quality: enforce ruff formatting, lint, import sorting; mypy strict for src package; pre-commit hooks; CI gates on style+tests. Provide Makefile targets: fmt, lint, typecheck, test, up, down, worker.","Containerization: Dockerfile for API (multi-stage, installs uv deps). docker-compose.yml with services: api, postgres (pgvector), redis, worker. Expose ports and healthchecks. Provide sample env file.","Migrations: set up Alembic configured for async SQLAlchemy. Provide initial migration to create items table with vector column and proper index (IVFFLAT/HNSW depending on pgvector version; default to HNSW if available; otherwise IVFFLAT). Document how to tune index parameters.","Performance defaults: use connection pooling settings, request timeouts, and embedding batch size env. Avoid global locks; ensure singleton model initialization is thread-safe under Uvicorn workers (use per-process init).","Deliverables: ensure repository contains all files listed in layout, runs locally with `docker compose up` + `uv run uvicorn ...`, and passes CI.","Do not leave TODO placeholders for core functionality; implement working minimal end-to-end: upsert item -> compute embedding -> store -> search returns results.","Keep code clean, readable, and modular; prefer explicitness over cleverness; add type hints across public functions."],"dependencies":{"runtime":["fastapi","uvicorn[standard]","pydantic-settings","sqlalchemy[asyncio]","asyncpg","psycopg[binary]","alembic","pgvector","redis","rq","sentence-transformers","torch","numpy","orjson","python-jose[cryptography]","passlib[bcrypt]","python-multipart","prometheus-client","structlog"],"dev":["ruff","mypy","pytest","pytest-asyncio","pytest-cov","httpx","pre-commit","types-redis","types-python-jose"],"optional":["grpcio","grpcio-tools","opentelemetry-api","opentelemetry-sdk","opentelemetry-instrumentation-fastapi","opentelemetry-instrumentation-logging","scikit-build-core","pybind11","cmake","ninja"]},"configuration":{"env_vars":["APP_ENV=development|production","APP_HOST=0.0.0.0","APP_PORT=8000","DATABASE_URL=postgresql+asyncpg://user:pass@postgres:5432/app","REDIS_URL=redis://redis:6379/0","JWT_SECRET=change_me","JWT_ALG=HS256","CORS_ORIGINS=* (dev default)","EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2","EMBEDDING_DIM=384 (match model)","PREFER_GPU=true","EMBED_BATCH_SIZE=32","ENABLE_GRPC=false","ENABLE_RERANK=false","CACHE_TTL_SECONDS=30"]},"acceptance_criteria":["`docker compose up` starts api, postgres+pgvector, redis, worker successfully with healthchecks passing.","GET /v1/health returns 200 with device info and db connectivity true.","POST /v1/items stores content and embedding (sync or async) and returns item id.","POST /v1/search returns relevant items ordered by similarity; top_k respected; filters supported.","Project runs on CPU-only machines and, when available, uses CUDA or MPS automatically without code changes.","All default tests pass via `make test` and CI workflow is present.","C++ extension builds successfully when optional toolchain is installed; pure Python fallback works when not built."]}